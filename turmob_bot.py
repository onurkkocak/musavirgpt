import requests
from bs4 import BeautifulSoup
import json
import time

# --- AYARLAR ---
# Senin verdiÄŸin tam adres
TARGET_URL = "https://www.turmob.org.tr/sirkuler/1/vergi"
BASE_URL = "https://www.turmob.org.tr"
OUTPUT_FILE = "musavirgpt_veri_seti.json"

def verileri_cek():
    print(f"ğŸŒ TÃœRMOB Vergi SirkÃ¼leri taranÄ±yor: {TARGET_URL}...")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
    }

    try:
        # 1. Ana Listeyi Ã‡ek
        # verify=False yapÄ±yoruz Ã§Ã¼nkÃ¼ bazen devlet sitelerinin SSL sertifikalarÄ± Python'da hata verebiliyor.
        response = requests.get(TARGET_URL, headers=headers, verify=False) 
        
        if response.status_code != 200:
            print(f"âŒ Siteye eriÅŸilemedi! Hata Kodu: {response.status_code}")
            return

        soup = BeautifulSoup(response.content, "html.parser")
        
        # Linkleri Bulma Stratejisi:
        # Sayfadaki tÃ¼m <a> etiketlerini al, iÃ§inde 'sirkuler' geÃ§enleri ayÄ±kla.
        veriler = []
        bulunan_linkler = []
        
        tum_linkler = soup.find_all("a", href=True)
        
        for link in tum_linkler:
            href = link['href']
            # Link filtreleme: Sadece detay sayfalarÄ±na gidenleri alalÄ±m
            # Genellikle '/sirkuler/' veya sayÄ±sal ID iÃ§erirler.
            if "/sirkuler/" in href and len(href) > 25: 
                full_url = BASE_URL + href if href.startswith("/") else href
                
                # MÃ¼kerrerleri ve ana sayfa linkini ele
                if full_url not in bulunan_linkler and full_url != TARGET_URL:
                    bulunan_linkler.append(full_url)

        print(f"âœ… Toplam {len(bulunan_linkler)} adet sirkÃ¼ler linki bulundu. Detaylar Ã§ekiliyor...")

        # Ä°lk 10 tanesini Ã§ekelim (Deneme amaÃ§lÄ±, sonra artÄ±rabilirsin)
        for i, url in enumerate(bulunan_linkler[:10]):
            try:
                print(f"   â³ Ä°ndiriliyor ({i+1}): {url}")
                detay_resp = requests.get(url, headers=headers, verify=False)
                detay_soup = BeautifulSoup(detay_resp.content, "html.parser")

                # --- BAÅLIK BULMA ---
                # GÃ¶rselde baÅŸlÄ±k mavi bantÄ±n iÃ§inde gÃ¶rÃ¼nÃ¼yor, muhtemelen h1 veya h2
                baslik = "BaÅŸlÄ±k Yok"
                header_tag = detay_soup.find("h1") or detay_soup.find("h2") or detay_soup.find("h3")
                if header_tag:
                    baslik = header_tag.get_text(strip=True)

                # --- Ä°Ã‡ERÄ°K (Ã–ZET) BULMA ---
                # Senin 2. gÃ¶rseldeki "Ã–ZET" kutusunu hedefliyoruz.
                icerik_metni = ""
                
                # 1. YÃ¶ntem: 'Ã–ZET' kelimesini iÃ§eren bir baÅŸlÄ±k var mÄ±?
                # Genellikle <div class="ozet"> veya <strong>Ã–ZET</strong> gibi olur.
                content_div = detay_soup.find("div", class_="news-detail") or detay_soup.find("div", {"id": "page-content"})
                
                if content_div:
                    icerik_metni = content_div.get_text(separator=" ", strip=True)
                else:
                    # Bulamazsa tÃ¼m paragraflarÄ± al
                    texts = [p.get_text(strip=True) for p in detay_soup.find_all("p")]
                    icerik_metni = " ".join(texts)

                # Temizlik ve Kontrol
                icerik_metni = icerik_metni.replace("\n", " ").replace("\r", "")
                
                if len(icerik_metni) > 50: # Sadece dolu olanlarÄ± kaydet
                    veriler.append({
                        "baslik": baslik,
                        "icerik": icerik_metni,
                        "kaynak": "TÃœRMOB SirkÃ¼leri",
                        "url": url
                    })
                
                time.sleep(0.5) # Kibar olalÄ±m, siteyi yormayalÄ±m

            except Exception as e:
                print(f"   âš ï¸ Bu linkte hata oldu: {e}")

        # Kaydet
        if len(veriler) > 0:
            with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                json.dump(veriler, f, ensure_ascii=False, indent=4)
            print(f"ğŸ‰ SÃœPER! Toplam {len(veriler)} adet sirkÃ¼ler '{OUTPUT_FILE}' dosyasÄ±na kaydedildi.")
        else:
            print("âš ï¸ HiÃ§ veri kaydedilemedi. Site yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir veya JavaScript engeli var.")

    except Exception as e:
        print(f"âŒ Kritik Hata: {e}")

if __name__ == "__main__":
    # SSL UyarÄ±larÄ±nÄ± gizle (gÃ¶rÃ¼ntÃ¼ kirliliÄŸi yapmasÄ±n)
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    verileri_cek()